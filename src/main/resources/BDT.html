








	
		
		<div id="session_8667" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=8667" class="openInPopup">
					
						<span class="abbreviation">BDT201 - </span>
					
					<span class="title">Big Data and HPC State of the Union</span>
				</a>
        
				
					<span class="abstract">Leveraging big data and high performance computing (HPC) solutions enables your organization to make smarter and faster decisions that influence strategy, increase productivity, and ultimately grow your business. We kick off the Big Data & HPC track with the latest advancements in data analytics, databases, storage, and HPC at AWS.&nbsp; Hear customer success stories and discover how to put data to work in your own organization.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Ben Butler &ndash; Senior Solutions Marketing Manager, Big Data and HPC with Amazon Web Services<br/>Ayumi Tada &ndash; Infrastructure Technology Department with Honda Motor Co., Ltd.<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9003" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9003" class="openInPopup">
					
						<span class="abbreviation">BDT202 - </span>
					
					<span class="title">HPC Now Means 'High Personal Computing'</span>
				</a>
        
				
					<span class="abstract">Since 2011, ONS.org.br (responsible for planning and operating the Brazilian Electric Sector) has been using AWS to run daily simulations using complex mathematical models. The use of the MIT StarCluster&nbsp; toolkit makes running HPC on AWS much less complex and lets ONS provision a high performance cluster in less than 5 minutes. Since the elapsed time of a big cluster depends of the user, ONS decide to develop a HPC portal where its engineers can interface with AWS and MIT StarCluster without knowing a line of code or having to use the command terminal. It is just a simple turn-on/turn-off portal. The cluster now gets personal, and every engineer runs the models using HPC on AWS as if they are using a PC.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Ricardo Geh &ndash; Enterprise Solutions Architect, Amazon Web Services with Amazon Web Services<br/>Sergio Mafra &ndash; IT Innovation Leader with ONS - Operador Nacional do Sistema Eletrico<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_8739" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=8739" class="openInPopup">
					
						<span class="abbreviation">BDT203 - </span>
					
					<span class="title">From Zero to NoSQL Hero: Amazon DynamoDB Tutorial</span>
				</a>
        
				
					<span class="abstract">Got data? Interested in learning about NoSQL? In this session, we take you from not knowing anything about Amazon DynamoDB to being able to build an advanced application on top of DynamoDB. We start with an overview of the service, basic fundamental concepts, and then dive right in to a hands-on follow along tutorial in which you: create your own table, make queries, add secondary indexes to existing tables, query against the secondary indexes, modify your indexes, as well as detect changes to your data in DynamoDB to build all kinds of analytics and complex event processing apps. You can walk in a novice with DynamoDB, but rest assured, you will walk out as a NoSQL expert ready to tackle large distributed systems problems with your database problems addressed with DynamoDB.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">David Yanacek &ndash; Sr. Software Dev Engineer with Amazon Web Services<br/>Jason Lambert &ndash; Senior Software Engineer with Here<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9222" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9222" class="openInPopup">
					
						<span class="abbreviation">BDT204 - </span>
					
					<span class="title">Rendering a Seamless Satellite Map of the World with AWS and NASA Data</span>
				</a>
        
				
					<span class="abstract">NASA imaging satellites deliver GB's of images to Earth every day. Mapbox uses AWS to process that data in real-time and build the most complete, seamless satellite map of the world. Learn how Mapbox uses Amazon S3 and Amazon SQS to stream data from NASA into clusters of EC2 instances running a clever algorithm that stiches images together in parallel. This session includes an in-depth discussion of high-volume storage with Amazon S3, cost-effecient data processing with Amazon EC2 Spot Instances, reliable job orchestration with Amazon SQS, and demand resilience with Auto Scaling.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Will White &ndash; Engineering with Mapbox<br/>Eric Gundersen &ndash; CEO with Mapbox<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_8853" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=8853" class="openInPopup">
					
						<span class="abbreviation">BDT205 - </span>
					
					<span class="title">Your First Big Data Application on AWS</span>
				</a>
        
				
					<span class="abstract">Want to get ramped up on how to use Amazon's big data web services and launch your first big data application on AWS? Join us on our journey as we build&nbsp;a big data application in real-time using&nbsp;Amazon EMR, Amazon Redshift, Amazon Kinesis, Amazon DynamoDB, and Amazon S3. We review architecture design patterns for big data solutions on AWS, and give you access to a take-home lab so that you can rebuild and customize the application yourself.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Matt Yanchyshyn &ndash; Principal Solutions Architect with Amazon Web Services<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9224" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9224" class="openInPopup">
					
						<span class="abbreviation">BDT206 - </span>
					
					<span class="title">See How Amazon Redshift is Powering Business Intelligence in the Enterprise</span>
				</a>
        
				
					<span class="abstract">Take a look into how NordstromRack.com | HauteLook and Nasdaq OMX are using Amazon Redshift for data warehouse and supporting business intelligence workloads one year after they made the move to using Amazon Redshift. &nbsp;We will cover why HauteLook chose Redshift, how they built the architecture, discuss what data is being stored and accessed, and overall, how that data is powering the HauteLook business. &nbsp;We will also&nbsp;discuss how Nasdaq migrated from an on-premised data warehouse to Amazon Redshift, and how they've been able to take advantage of Redshift's array of security features such as hardware security modules (HSM), encryption, and audit-logging.&nbsp;
&nbsp;</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Rahul Pathak &ndash; Principal Product Manager with Amazon Web Services<br/>Jason Timmes &ndash; Associate VP of Software Development with Nasdaq OMX<br/>Kevin Diamond &ndash; Chief Technology Officer, Nordstromrack.com | HauteLook with NordstromRack.com | HauteLook<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9865" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9865" class="openInPopup">
					
						<span class="abbreviation">BDT207 - </span>
					
					<span class="title">Use Streaming Analytics to Exploit Perishable Insights</span>
				</a>
        
				
					<span class="abstract">Streaming analytics is about knowing and acting on what&rsquo;s happening in your business and with your customers right this second. Forrester calls these perishable insights because they occur at a moment&rsquo;s notice and you must act on them fast. The high velocity, whitewater flow of data from innumerable real-time data sources such as market data, internet of things, mobile, sensors, clickstream, and even transactions remain largely un-navigated by most firms. The opportunity to leverage streaming analytics has never been greater. In this session, Forrester analyst Mike Gualtieri explains the opportunity, use cases, and how to use cloud-based streaming solutions in your application architecture.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Mike Gualtieri &ndash; Principal Analyst with Forrester Research<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_10833" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=10833" class="openInPopup">
					
						<span class="abbreviation">BDT208 - </span>
					
					<span class="title">Finding High Performance in the Cloud for HPC</span>
				</a>
        
				
					<span class="abstract">&nbsp;
Hear how high performance for HPC workloads can be found in the cloud. In this panel session, Nicole Hemsoth, Editor in Chief of Cloud Insights and of HPCWire, hosts a lively discussion with experts who address the reality that big data and high performance computing performance cannot be sacrificed. They explore how HPC users have been finding the AWS cloud a cost-effective way to get large-scale, performance-sensitive jobs done in a fraction of the time and without the heavy lifting required to maintain on-premises systems. Users are always looking for more (and more interesting) ways to implement their HPC jobs in the cloud. From working with GPU-powered instances to instances with more memory or capability, panelists explore what performance really means in the cloud, how on-site, traditional supercomputing compares to the cloud, and what the future may be for HPC end users with cloud services. Sponsored by Intel.
&nbsp;</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">David Pellerin &ndash; Principal Business Development Manager, EC2 with Amazon Web Services<br/>Ray Milhem &ndash; Vice President with ANSYS, Inc.<br/>Ayumi Tada &ndash; Infrastructure Technology Department with Honda Motor Co., Ltd.<br/>Nicole Hemsoth &ndash; Editor in Chief, Cloud Insights and HPCWire with IDG<br/>Debra Goldfarb &ndash; Chief Analyst, Data Center Division and Senior Director of Market Intelligence with Intel<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_11329" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=11329" class="openInPopup">
					
						<span class="abbreviation">BDT209 - </span>
					
					<span class="title">Intel’s Healthcare Cloud Solution Using Wearables for Parkinson’s Disease Research</span>
				</a>
        
				
					<span class="abstract">In this session, learn how the Intel team of software engineers and data scientists, in collaboration with the Michael J Fox Foundation, built a big data analytics platform &nbsp;using Hadoop and other IoT technologies. The solution&nbsp;leverages wearable sensors and smartphone application to monitor PD patient&rsquo;s motor activities, 24/7. The platform collects and processes large stream of data, and enables different analytics services such as activity recognition and different PD related measurements to researchers. &nbsp;These machine learning algorithms are used to &nbsp;detect patterns in the data that can help researchers understand the progression of the disease and develop effective treatments. You leave with a comprehensive view of the tools and platforms from Intel that you can use in building your own applications on AWS. In addition there will be a deeper dive to explain the way this platforms &nbsp;enables near real- time analytics as part of the ingestion process.&nbsp;
 
Parkinson&rsquo;s Disease&mdash;a neuromuscular disease that causes gradually worsening symptoms such as tremors, difficulty in movement, and sleep loss &mdash;affects over 5 million people worldwide. Because the symptoms vary from individual to individual, research into the disease is hampered by the lack of objective data. As is typical of many healthcare applications, the collection, storage, and analysis of data is complex, expensive, and time-consuming. Intel is tackling this challenge by building a solution that uses wearable devices to collect data from patients anonymously and store it securely.
Sponsored by Intel.
&nbsp;</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Moty Fania &ndash; Principal Architect with Intel<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_8676" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=8676" class="openInPopup">
					
						<span class="abbreviation">BDT302 - </span>
					
					<span class="title">Big Data Beyond Hadoop: Running Mahout, Giraph, and R on Amazon EMR and Analyzing Results with Amazon Redshift</span>
				</a>
        
				
					<span class="abstract">We will explore the strengths and limitations of Hadoop for analyzing large data sets and review the growing ecosystem of tools for augmenting, extending, or replacing Hadoop MapReduce.&nbsp; We will introduce the Amazon Elastic MapReduce (EMR) platform as the big data foundation for Hadoop and beyond by providing specific examples of running Machine Learning (Mahout), Graph Analytics (Giraph), and Statistical Analysis (R) on EMR. We will discuss also big data analytics and visualization of results with Amazon Redshift + third party business intelligence tools, as well as typical end-to-end Big Data workflow on AWS.
We will conclude with real-world examples from ICAO of Big Data analytics for aviation safety data on AWS. The integrated Safety Trend Analysis and Reporting System (iSTARS) is a web based system linking a collection of safety datasets and related web application to perform online safety and risk analysis. It uses AWS EC2, S3, EMR and related partner tools for continuous data aggregation and filtering.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Nikolay Bakaltchev &ndash; Senior Solutions Architect with Amazon Web Services<br/>Marco Merens &ndash; Acting Chief  Integrated Aviation Analysis with ICAO<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9285" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9285" class="openInPopup">
					
						<span class="abbreviation">BDT303 - </span>
					
					<span class="title">Construct Your ETL Pipeline with AWS Data Pipeline, Amazon EMR, and Amazon Redshift</span>
				</a>
        
				
					<span class="abstract">An advantage to leveraging Amazon Web Services for your data processing and warehousing use cases is the number of services available to construct complex, automated architectures easily. Using AWS Data Pipeline, Amazon EMR, and Amazon Redshift, we show you how to build a fault-tolerant, highly available, and highly scalable ETL pipeline and data warehouse. &nbsp;Coursera will show how they built their pipeline, and share best practices from their architecture.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Roy Ben-Alta &ndash; Big Data Analytics Practice Lead at Amazon Web Services - NA with Amazon Web Services<br/>P. Thomas Barthelemy &ndash; Software Engineer with Coursera<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9016" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9016" class="openInPopup">
					
						<span class="abbreviation">BDT305 - </span>
					
					<span class="title">Lessons Learned and Best Practices for Running Hadoop on AWS</span>
				</a>
        
				
					<span class="abstract">Enterprises are starting to deploy large scale Hadoop clusters to extract value out of the data that they are generating. These clusters often span hundreds of nodes. To speed up the time to value, a lot of the newer deployments are happening in AWS, moving from the traditional on-premises, bare-metal world. Cloudera supports just such deployments. In this session, Cloudera shares the lessons learned and best practices for deploying multi-tenant Hadoop clusters in AWS. They will cover what reference deployments look like, what services are relevant for Hadoop deployments, network configurations, instance types, backup and disaster recovery considerations, and security considerations. They will also talk about what works well, what doesn&rsquo;t, and what has to be done going forward to improve the operability of Hadoop on AWS.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Rahul Bhartia &ndash; Ecosystem Solution Architect with Amazon Web Services<br/>Amandeep Khurana &ndash; Principal Solutions Architect with Cloudera Inc<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_8589" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=8589" class="openInPopup">
					
						<span class="abbreviation">BDT306 - </span>
					
					<span class="title">Mission-Critical Stream Processing with Amazon EMR and Amazon Kinesis</span>
				</a>
        
				
					<span class="abstract">Organizations processing mission critical high-volume data must be able to achieve high levels of throughput and durability in data processing workflows. In this session, we will learn how DataXu is using Amazon Kinesis, Amazon S3, and Amazon EMR for its patented approach to programmatic marketing. Every second, the DataXu Marketing Cloud processes over 1 Million ad requests and makes more than 40 billion decisions to select and bid on ad impressions that are most likely to convert. In addition to addressing the scalability and availability of the platform, we will explore Amazon Kinesis producer and consumer applications that support high levels of scalability and durability in mission-critical record processing.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Ricardo DeMatos &ndash; Solutions Architect with Amazon Web Services<br/>Derek Chiles &ndash; Manager, Solutions Architecture with Amazon Web Services<br/>Yekesa Kosuru &ndash; V.P Engineering with DataXu<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_8774" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=8774" class="openInPopup">
					
						<span class="abbreviation">BDT307 - </span>
					
					<span class="title">Running NoSQL on Amazon EC2</span>
				</a>
        
				
					<span class="abstract">Deploying self-managed NoSQL databases on Amazon Web Services (AWS) is more straightforward than you would think. This session focuses on three popular self-managed NoSQL systems on Amazon EC2: MongoDB, Cassandra, and Couchbase. We start with an overview of each of these popular NoSQL databases, discuss their origins and characteristics, and demonstrate the use of the AWS ecosystem to deploy these NoSQL databases quickly. Later in the session, we dive deep on use cases, design patterns, and discuss creating highly-available and high-performance architectures with careful consideration for failure and recovery.&nbsp; Whether you're a NoSQL developer, architect, or administrator, join us for a comprehensive session on looking at three different NoSQL systems from a uniform perspective.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Matt Yanchyshyn &ndash; Principal Solutions Architect with Amazon Web Services<br/>Rahul Bhartia &ndash; Ecosystem Solution Architect with Amazon Web Services<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9282" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9282" class="openInPopup">
					
						<span class="abbreviation">BDT308 - </span>
					
					<span class="title">Using Amazon Elastic MapReduce as Your Scalable Data Warehouse</span>
				</a>
        
				
					<span class="abstract">In this presentation, we will demonstrate how to use Amazon Elastic MapReduce as your scalable data warehouse.&nbsp;Amazon EMR supports clusters with thousands of nodes and is used to access petabyte scale data&nbsp;warehouses. Amazon EMR is not only fast, but it is also easy to use for rapid development and adhoc analysis.&nbsp; We will show you how access the large scale data warehouses with emerging tools such as Hue, Hive, low latency SQL applications like Presto, and alternative execution engines like Apache Spark. We will also show you how these tools integrate directly with other AWS big data services such as Amazon S3, Amazon DynamoDB, and Amazon Kinesis.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Steve McPherson &ndash; Senior Manager, Elastic MapReduce with Amazon Web Services<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_11711" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=11711" class="openInPopup">
					
						<span class="abbreviation">BDT308-JT - </span>
					
					<span class="title">Using Amazon Elastic MapReduce as Your Scalable Data Warehouse - Japanese Track</span>
				</a>
        
				
					<span class="abstract">In this presentation, we will demonstrate how to use Amazon Elastic MapReduce as your scalable data warehouse.&nbsp;Amazon EMR supports clusters with thousands of nodes and is used to access petabyte scale data&nbsp;warehouses. Amazon EMR is not only fast, but it is also easy to use for rapid development and adhoc analysis.&nbsp; We will show you how access the large scale data warehouses with emerging tools such as Hue, Hive, low latency SQL applications like Presto, and alternative execution engines like Apache Spark. We will also show you how these tools integrate directly with other AWS big data services such as Amazon S3, Amazon DynamoDB, and Amazon Kinesis.
This is a repeat session that will be translated simultaneously into Japanese.
&nbsp;</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Steve McPherson &ndash; Senior Manager, Elastic MapReduce with Amazon Web Services<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_10645" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=10645" class="openInPopup">
					
						<span class="abbreviation">BDT309 - </span>
					
					<span class="title">Delivering Results with Amazon Redshift, One Petabyte at a Time</span>
				</a>
        
				
					<span class="abstract">The Amazon Enterprise Data Warehouse team, responsible for data warehousing across all of Amazon&rsquo;s divisions, spent 2014 working with Amazon Redshift on its largest datasets, including web log traffic. The key goals in this project were to provide a viable, enterprise-grade solution that enabled full scans of 2 trillion rows in under an hour at load. Key to success were automation of routine DW tasks that become complicated at scale: backfilling erroneous data, re-calculating statistics, re-sorting daily additions, and so forth. In this session, we discuss the scale and performance of a 100-node 1PB Amazon Redshift cluster, as well as describing some of the technical aspects and best practices of running 100-node clusters in an enterprise environment.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Samar Sodhi &ndash; Manager, Data Engineering with Amazon Web Services<br/>Erik Selberg &ndash; Dir, Amazon Data Warehouse with Amazon Web Services<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_11964" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=11964" class="openInPopup">
					
						<span class="abbreviation">BDT309-JT - </span>
					
					<span class="title">Delivering Results with Amazon Redshift, One Petabyte at a Time - Japanese Track</span>
				</a>
        
				
					<span class="abstract">The Amazon Enterprise Data Warehouse team, responsible for data warehousing across all of Amazon&rsquo;s divisions, spent 2014 working with Amazon Redshift on its largest datasets, including web log traffic. The key goals in this project were to provide a viable, enterprise-grade solution that enabled full scans of 2 trillion rows in under an hour at load. Key to success were automation of routine DW tasks that become complicated at scale: backfilling erroneous data, re-calculating statistics, re-sorting daily additions, and so forth. In this session, we discuss the scale and performance of a 100-node 1PB Amazon Redshift cluster, as well as describing some of the technical aspects and best practices of running 100-node clusters in an enterprise environment.
This is a repeat session that will be translated simultaneously into Japanese.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Samar Sodhi &ndash; Manager, Data Engineering with Amazon Web Services<br/>Erik Selberg &ndash; Dir, Amazon Data Warehouse with Amazon Web Services<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9478" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9478" class="openInPopup">
					
						<span class="abbreviation">BDT310 - </span>
					
					<span class="title">Big Data Architectural Patterns and Best Practices on AWS</span>
				</a>
        
				
					<span class="abstract">The world is producing an ever increasing volume, velocity, and variety of big data. Consumers and businesses are demanding up-to-the-second (or even millisecond) analytics on their fast-moving data, in addition to classic batch processing. AWS delivers many technologies for solving big data problems. But what services should you use, why, when, and how? In this session, we simplify big data processing as a data bus comprising various stages: ingest, store, process, and visualize. Next, we discuss how to choose the right technology in each stage based on criteria such as data structure, query latency, cost, request rate, item size, data volume, durability, and so on. Finally, we provide reference architecture, design patterns, and best practices for assembling these technologies to solve your big data problems&nbsp;at the right cost.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Siva Raghupathy &ndash; Principal Solutions Architect with Amazon Web Services<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9885" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9885" class="openInPopup">
					
						<span class="abbreviation">BDT311 - </span>
					
					<span class="title">MegaRun: Behind the 156,000 Core HPC Run on AWS and Experience of On-demand Clusters for Manufacturing Production Workloads</span>
				</a>
        
				
					<span class="abstract">Not only did the 156,000+ core run (nicknamed the MegaRun) on Amazon EC2 break industry records for size, scale, and power, but it also delivered real-world results. The University of Southern California ran the high-performance computing job in the cloud to evaluate over 220,000 compounds&nbsp;and build a better organic solar cell. In this session, USC provides an update&nbsp;on the six promising compounds that we have found and is now synthesizing in laboratories for&nbsp;a clean energy project. We discuss the implementation of and lessons learned in running a cluster in eight AWS regions worldwide, with highlights&nbsp;from Cycle Computing&rsquo;s project Jupiter, a low-overhead cloud scheduler and workload manager. This session also looks at how the MegaRun was financially achievable using the Amazon EC2 Spot Instance market, including an in-depth discussion on leveraging Spot Instances, a strategy to deal with the&nbsp;variability of Spot pricing, and a template to avoid compromising workflow integrity, security, or management.
After a year of production workloads on AWS, HGST, a Western Digital Company, has zeroed in on understanding how to create on-demand clusters to&nbsp;maximize&nbsp;value on AWS. HGST will outline the company's successes in addressing the company's changes in operations, culture, and behavior to this new vision of on-demand clusters. In addition, the session will provide insights into leveraging Amazon EC2 Spot Instances to reduce costs and maximize value, while maintaining the needed flexibility, and agility that AWS is known for.&quot;
&nbsp;</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Jason Stowe &ndash; CEO with Cycle Computing<br/>Patrick Saris &ndash; Chemist with University of Southern California<br/>David Hinz &ndash; Global Director, Cloud, Data Center, Computing Engineering with HGST<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_10852" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=10852" class="openInPopup">
					
						<span class="abbreviation">BDT312 - </span>
					
					<span class="title">Using the Cloud to Scale from a Database to a Data Platform</span>
				</a>
        
				
					<span class="abstract">Scaling highly available database infrastructure to 100x, 1000x, and beyond has historically been one of the hardest technical challenges that any successful web business must face. This is quickly changing with fully-managed database services such as Amazon DynamoDB and Amazon Redshift, as the scaling efforts which previously required herculean effort&nbsp;are now as simple as an API call.
&nbsp;
Over the last few years, Twilio has evolved their database infrastructure to a pipeline consisting of Amazon SQS, Sharded MySQL, Amazon DynamoDB, Amazon S3, Amazon EMR and Amazon Redshift. In this session, Twilio cover show they achieved success, specifically:
- How they replaced their data pipeline deployed to Amazon EC2 to meet their scaling needs with zero downtime. 
- How they adopted Amazon DynamoDB and Amazon Redshift at the same scale as their MySQL infrastructure, at 1/5th the cost and operational overhead.
- Why they believe adopting managed database services like Amazon DynamoDB is key to accelerating delivery of value to their customers.
Sponsored by Twilio.
&nbsp;</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Ryan Horn &ndash; Technical Lead, User Data with Twilio<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_8590" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=8590" class="openInPopup">
					
						<span class="abbreviation">BDT401 - </span>
					
					<span class="title">Big Data Orchestra - Harmony within Data Analysis Tools</span>
				</a>
        
				
					<span class="abstract">Yes, you can build a data analytics solution with a relational database, but should you? What about scalability? What about flexibility? What about cost? In this session, we demonstrate how to build a real world solution for location-based data analytics, with the combination of Amazon Kinesis, Amazon DynamoDB, Amazon Redshift, Amazon CloudSearch, and Amazon EMR. We discuss how to integrate these services to create a robust solution in terms of security, simplicity, speed, and low cost.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Guy Ernest &ndash; Sr. Manager Solutions Architecture with Amazon Web Services<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_8758" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=8758" class="openInPopup">
					
						<span class="abbreviation">BDT402 - </span>
					
					<span class="title">Performance Profiling in Production: Analyzing Web Requests at Scale Using Amazon Elastic MapReduce and Storm</span>
				</a>
        
				
					<span class="abstract">Code profiling gives a rich, detailed view of runtime performance. However, it's difficult to achieve in production: for even a small fraction of web requests, huge challenges in scalability, access, and ease of use appear. Despite this, Yelp profiles a nontrivial fraction of its traffic by combining Amazon EC2, Amazon EMR, and Amazon S3. Developers can search, sort, filter, and combine interesting profiles; during a site slowdown or page failure, this allows a fast diagnosis and speedy recovery. Some of our analyses run nightly, while others run in real-time via Storm topologies. This session includes our use cases for code profiling, its benefits, and the implementation of its handlers and analysis flows. We include both performance results and implementation challenges of our MapReduce and Storm jobs, including code overviews.&nbsp; We also touch on issues such as concurrent logging, cross-data center replication, job scheduling, and API definitions.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Zach Musgrave &ndash; Software Engineer with Yelp, Inc.<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	
		
		<div id="session_9864" class="resultRow sessionRow ">
			<div class="detailColumn">
				
				<a href="sessionDetail.ww?SESSION_ID=9864" class="openInPopup">
					
						<span class="abbreviation">BDT403 - </span>
					
					<span class="title">Netflix's Next Generation Big Data Platform</span>
				</a>
        
				
					<span class="abstract">As&nbsp;Netflix&nbsp;expands their services to more countries, devices, and content, they continue to evolve their big data analytics platform to&nbsp;accommodate&nbsp;the increasing needs of product and consumer insights. This year,&nbsp;Netflix&nbsp;re-innovated their big data platform: they&nbsp;upgraded to Hadoop 2, transitioned to the Parquet file format,&nbsp;experimented with Pig on Tez for the ETL workload, and adopted Presto as their interactive querying engine.&nbsp; In this session,&nbsp;Netflix&nbsp;discusses their latest architecture, how they built it on the Amazon EMR infrastructure, the contributions put into the open source community, as well as some performance numbers for running a big data warehouse with Amazon S3.</span>
				
				
					<small class="length">45 minutes</small>
				
				
					<small class="type">Breakout Session</small>
				
				
					<small class="speakers">Eva Tse &ndash; Director of Big Data Platform with Netflix<br/></small>
				
				<span class="track"></span>
				<span class="scheduleStatus">
					
					
					
					
				</span>
			</div>
			<div class="actionColumn">
				


				
					
				
				
			</div>
		</div>
	


	

<div id="downloadDocsDialog" title="Available Docs"></div>

	
<script type="text/javascript" charset="utf-8">
	
		//update search quantities
		updateSearchCount({
			attendee: '',
			session: '24',
			speaker: '430',
			exhibitor: '',
			file: '0'
		});
	
	$(function(){
		sessionTooltip();
		downloadDocDialogInit();
		ratingInit();
	});
</script>
